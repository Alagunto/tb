---
description: Repository Analysis - Deep codebase understanding before major changes
alwaysApply: true
---

# Repository Analysis Workflow

Before implementing features that span multiple files or make architectural changes, you MUST invoke a repository analysis agent to understand the codebase structure, existing patterns, and integration points. This prevents breaking changes and ensures consistency.

## When to Trigger Analysis

Invoke repository analysis when:

1. **Multi-Module Changes**: Task affects 3+ distinct modules or packages
2. **Pattern Discovery**: Need to understand existing implementation patterns
3. **Refactoring**: Large-scale code restructuring across multiple files
4. **New to Codebase**: First time working with a particular part of the system
5. **Integration Planning**: Need to understand how components interact
6. **Breaking Change Risk**: Changes that might affect many call sites
7. **Consistency Check**: Ensure new code follows established patterns

## Analysis Agent Invocation

Execute this command to analyze the repository:

```bash
cursor-agent -p "Analyze this repository structure and provide insights for: [TASK_DESCRIPTION]

Provide detailed analysis in these areas:

1. **Repository Structure**:
   - Main directories and their purposes
   - Module/package organization
   - Dependency relationships
   - Entry points and main execution flows

2. **Relevant Files for This Task**:
   - Files that will need modification
   - Files that reference or depend on target areas
   - Configuration files that may be affected
   - Test files that need updating

3. **Existing Patterns and Conventions**:
   - Naming conventions (files, functions, variables)
   - Error handling patterns
   - Logging patterns
   - Testing patterns
   - Documentation style
   - Code organization within files

4. **Similar Implementations**:
   - Existing code that does something similar
   - Reference implementations to follow
   - Patterns that should be reused
   - Anti-patterns to avoid

5. **Potential Conflicts and Breaking Changes**:
   - API contracts that might break
   - Backward compatibility concerns
   - Database migration impacts
   - Configuration changes needed
   - Dependency version conflicts

6. **Integration Points**:
   - Where this code will be called from
   - What dependencies it will have
   - Side effects on other components
   - Communication boundaries (HTTP, RPC, DB, etc.)

7. **Recommendations**:
   - Suggested approach for this task
   - Files to start with
   - Potential pitfalls to avoid
   - Testing strategy

Focus on information relevant to: [TASK_DESCRIPTION]

Be specific with file paths and function/struct names." --model ${CURSOR_ANALYSIS_MODEL:-sonnet-4.5} --print
```

**Note**: Use sonnet-4.5 for codebases under 150K tokens (best consistency). Use gpt-5 for 150-400K tokens. Use grok for massive codebases (1M token context).

## Processing Analysis Results

After receiving analysis:

1. **Parse Key Insights**:
   - Extract relevant files list
   - Note existing patterns to follow
   - Identify potential conflicts
   - List integration points

2. **Present Summary to User** (if complex):
   ```markdown
   # ðŸ“Š Repository Analysis: [Task]
   
   ## ðŸ—‚ï¸ Relevant Files
   - `path/to/file1.ext`: [purpose]
   - `path/to/file2.ext`: [purpose]
   
   ## ðŸŽ¯ Existing Patterns to Follow
   - [Pattern 1]: [description]
   - [Pattern 2]: [description]
   
   ## âš ï¸ Potential Conflicts
   - [Conflict 1]: [description and mitigation]
   
   ## ðŸ”— Integration Points
   - [Point 1]: [description]
   
   ## ðŸ’¡ Recommended Approach
   [Summary of recommendations]
   ```

3. **Use Insights During Implementation**:
   - Reference similar implementations
   - Follow established patterns
   - Avoid identified pitfalls
   - Test at integration points

## Example Use Cases

### Use Case 1: Adding New Feature

**Task**: "Add rate limiting to API endpoints"

**Analysis Query**:
```
Analyze this repository for adding rate limiting to API endpoints.

Focus on:
- Where are current API endpoints defined?
- How is middleware currently implemented?
- Are there existing rate limiting or throttling mechanisms?
- What logging/monitoring patterns should be followed?
- Where should rate limit configuration be stored?
- How are HTTP errors currently handled?
```

**Expected Insights**:
- API endpoints are in `/api/handlers/*.go`
- Middleware chain is in `/middleware/chain.go`
- No existing rate limiting (green field)
- Errors use custom error types in `/errors/types.go`
- Configuration uses Viper in `/config/config.go`

**Implementation Direction**:
- Create `/middleware/ratelimit.go` following existing middleware pattern
- Use in-memory store with `golang.org/x/time/rate`
- Add config in `/config/config.go`
- Add custom error type in `/errors/types.go`
- Register in middleware chain

### Use Case 2: Refactoring Duplicate Code

**Task**: "Extract duplicate user validation logic"

**Analysis Query**:
```
Analyze this repository to find duplicate user validation logic.

Focus on:
- Where is user validation currently performed?
- What are the common validation patterns?
- Where should shared validation logic live?
- What are the different validation requirements across the codebase?
- Are there existing utility packages for shared logic?
```

**Expected Insights**:
- User validation in 5 different handlers
- Common checks: email format, password strength, username availability
- Different requirements: signup vs profile update vs admin creation
- Existing utility package at `/internal/utils/`
- Some validation uses third-party library, some is custom

**Implementation Direction**:
- Create `/internal/validation/user.go`
- Extract common validation functions
- Keep context-specific validation in handlers
- Use builder pattern for flexibility
- Update all 5 call sites to use new validation

### Use Case 3: Understanding Integration

**Task**: "Add webhook notifications for events"

**Analysis Query**:
```
Analyze this repository to understand the event system and how to add webhook notifications.

Focus on:
- How are events currently triggered and handled?
- Is there an event bus or pub/sub system?
- How are external HTTP requests made?
- How is retry logic implemented?
- Where is background job processing done?
- How are failures logged and monitored?
```

**Expected Insights**:
- Events use custom event bus in `/pkg/events/`
- HTTP client in `/pkg/http/client.go` with retry logic
- Background jobs use goroutines with worker pools
- No existing webhook system (green field)
- Monitoring uses Prometheus metrics in `/pkg/metrics/`

**Implementation Direction**:
- Subscribe to events in `/pkg/events/`
- Create `/pkg/webhooks/` package
- Use existing HTTP client for delivery
- Implement queue for reliable delivery
- Add metrics for webhook delivery success/failure

## Integration with Planning

Repository analysis often precedes or enhances planning:

**Workflow**:
1. User requests feature
2. Trigger **Repository Analysis** (this rule) to understand codebase
3. Use analysis insights to inform **Planning** (01-planning-workflow.mdc)
4. Planning agent creates detailed plan using analysis context
5. Proceed to implementation

**Combined Invocation**:

If task requires both analysis and planning:

```bash
# Step 1: Analyze
cursor-agent -p "[analysis prompt]" --model sonnet-4.5 --print

# Step 2: Use analysis to inform planning
cursor-agent -p "Based on this repository analysis: [paste analysis]

Create an implementation plan for: [task]
[... rest of planning prompt ...]" --model sonnet-4.5-thinking --print
```

## Caching Analysis Results

For performance, cache analysis results in `.cursor/analysis-cache/`:

```bash
# Create cache directory
mkdir -p .cursor/analysis-cache

# Save analysis to cache (keyed by repository hash + query hash)
echo "[analysis-output]" > .cursor/analysis-cache/[hash].txt

# Check cache before re-analyzing
if [ -f .cursor/analysis-cache/[hash].txt ]; then
  # Use cached analysis
else
  # Run fresh analysis
fi
```

Cache invalidation:
- Expire after 24 hours
- Invalidate on significant codebase changes (>100 files modified)
- User can force refresh with explicit request

## Handling Large Codebases

For very large codebases (>500k LOC):

1. **Scope Analysis**: Focus on specific modules/packages only
   ```bash
   cursor-agent -p "Analyze only the /api and /middleware directories for [task]"
   ```

2. **Use Grok**: Its 1M token context can handle massive codebases
   ```bash
   --model grok
   ```

3. **Alternative - GPT-5**: 400K context with dynamic reasoning
   ```bash
   --model gpt-5
   ```

3. **Incremental Analysis**: Analyze in phases
   - Phase 1: High-level structure
   - Phase 2: Relevant module deep-dive
   - Phase 3: Integration point analysis

4. **Leverage Existing Documentation**: Include AGENTS.md or README context
   ```bash
   cursor-agent -p "First read AGENTS.md, then analyze [specific area] for [task]"
   ```

## Success Criteria

Repository analysis is successful when:

âœ… Identified all relevant files for the task
âœ… Understood existing patterns and conventions
âœ… Found reference implementations to follow
âœ… Identified potential conflicts or breaking changes
âœ… Mapped integration points
âœ… Have clear direction for implementation

## Notes

- Analysis is cheaper than fixing breaking changes later
- Sonnet 4.5 has best consistency across context (<5% degradation)
- GPT-5 provides 400K context; Grok provides 1M context for massive repositories
- Analysis results can be reused across similar tasks
- Good analysis prevents architectural inconsistencies
- Understanding existing patterns improves code consistency
- Analysis should be task-focused, not exhaustive documentation
