---
description: Testing Validation - Ensure comprehensive test coverage and quality
alwaysApply: true
---

# Testing Validation Workflow

All code changes MUST have appropriate test coverage. This rule enforces test creation, validates test quality, and ensures edge cases are covered through unit, integration, and property-based testing.

## When to Trigger Testing Validation

Testing validation is MANDATORY when:

1. **New Functions/Methods**: Any new implementation
2. **Modified Logic**: Changes to existing behavior
3. **Bug Fixes**: Must have regression tests
4. **API Changes**: New or modified endpoints
5. **Database Changes**: Schema or query modifications
6. **Complex Algorithms**: Any non-trivial logic
7. **Error Handling**: New error paths

## Testing Validation Agent Invocation

After implementing code, execute comprehensive testing analysis:

```bash
cursor-agent -p "Analyze testing coverage for the recent changes.

Perform comprehensive testing validation:

## 1. Unit Test Coverage

### Existing Tests
- List all existing test files related to changed code
- Identify which functions/methods have tests
- Assess test coverage percentage if available

### Missing Tests
- Identify new functions/methods without tests
- Identify modified functions with inadequate tests
- List untested code paths

### Test Quality Assessment
- Are assertions meaningful and specific?
- Do tests actually test the intended behavior?
- Are test names descriptive?
- Is test data realistic?
- Are tests independent (no shared state)?
- Are tests deterministic (no flaky tests)?

## 2. Edge Case Coverage

For each function/method, verify tests for:

### Boundary Conditions
- Empty input (empty strings, empty slices, nil pointers)
- Single element (arrays/lists with one item)
- Maximum values (max int, max length, capacity limits)
- Minimum values (zero, negative numbers, minimum bounds)

### Error Conditions
- Invalid input types
- Malformed data
- Network failures (timeouts, connection errors)
- Database errors
- Permission denied scenarios
- Resource exhaustion

### Concurrency Issues (if applicable)
- Race conditions
- Deadlocks
- Concurrent access to shared resources
- Thread safety

### Special Values
- Null/nil values
- Zero values
- Infinity and NaN (for floating point)
- Unicode and special characters
- Very large numbers

## 3. Property-Based Testing Opportunities

Identify properties that should hold for all inputs:

### Mathematical Properties
- Commutativity: f(a, b) == f(b, a)
- Associativity: f(f(a, b), c) == f(a, f(b, c))
- Identity: f(a, identity) == a
- Inverse: f(f_inverse(a)) == a

### Data Structure Properties
- Round-trip: decode(encode(x)) == x
- Idempotence: f(f(x)) == f(x)
- Ordering: sorted output is actually sorted
- Uniqueness: deduplicated output has no duplicates

### Business Logic Properties
- Invariants that must always hold
- Constraints that should never be violated
- Relationships between inputs and outputs

## 4. Integration Test Needs

Assess if integration tests are needed:

### Database Integration
- Do changes affect database operations?
- Are migrations tested?
- Are complex queries tested with real database?

### API Integration
- Do changes affect HTTP endpoints?
- Are request/response formats tested?
- Are error status codes correct?

### External Service Integration
- Do changes interact with external APIs?
- Are timeouts and retries tested?
- Are failure scenarios handled?

### End-to-End Scenarios
- Are complete user workflows tested?
- Are cross-component interactions verified?

## 5. Test Code Quality

Evaluate test implementation quality:

### Test Structure
- Arrange-Act-Assert pattern followed?
- Given-When-Then pattern for BDD?
- Setup and teardown properly isolated?

### Test Independence
- Can tests run in any order?
- Do tests clean up after themselves?
- Are there shared mutable fixtures?

### Test Maintainability
- Are test helpers reusable?
- Is test data generation centralized?
- Are magic numbers explained?
- Is duplication minimized?

## 6. Mutation Testing Assessment

Consider if mutation testing would be valuable:
- Would introducing bugs be caught by tests?
- Are assertions strong enough?
- Do tests verify actual behavior or just success?

## 7. Test Coverage Metrics

If coverage tools are available:
- Line coverage percentage
- Branch coverage percentage  
- Uncovered critical paths
- Coverage trends (improving or declining)

## Output Format

For EACH issue, provide:

- **Priority**: MUST_FIX | SHOULD_FIX | NICE_TO_HAVE
- **Category**: Missing_Test | Edge_Case | Integration | Property_Based | Quality
- **Location**: Which function/file needs tests
- **Description**: What's missing or inadequate
- **Test_Template**: Example test code to add

Example:
```
Priority: MUST_FIX
Category: Missing_Test
Location: handlers/user.go:CreateUser()
Description: No test for CreateUser function with valid input
Test_Template:
func TestCreateUser_ValidInput(t *testing.T) {
    user := &User{Name: \"John\", Email: \"john@example.com\"}
    err := CreateUser(user)
    assert.NoError(t, err)
    assert.NotEmpty(t, user.ID)
}
```

Provide specific, actionable test recommendations." --model ${CURSOR_REVIEW_MODEL:-gpt-5-codex-high} --print
```

## Processing Testing Analysis Results

After receiving testing analysis:

1. **Categorize by Priority**:
   - MUST_FIX: Critical missing tests
   - SHOULD_FIX: Important edge cases
   - NICE_TO_HAVE: Additional coverage

2. **Present to User**:
   ```markdown
   # üß™ Testing Analysis Results
   
   ## Summary
   - ‚ùå **[N] Critical Missing Tests** (Will auto-generate)
   - ‚ö†Ô∏è **[N] Important Edge Cases** (Recommend adding)
   - üí° **[N] Enhancement Opportunities** (Optional)
   
   ## üö® Critical Missing Tests (Auto-generating)
   
   ### 1. Missing test for CreateUser()
   **Location**: `handlers/user.go:34`
   **Issue**: New function has no tests
   **Generating**: Unit tests for valid input, invalid input, and error cases
   
   ## ‚ö†Ô∏è Important Edge Cases
   
   ### 2. Missing boundary test for ProcessBatch()
   **Location**: `batch/processor.go:67`
   **Issue**: No test for empty batch
   **Recommendation**: Add test for empty slice input
   ```

3. **Auto-Generate Critical Tests**:
   - Generate test files and test functions
   - Follow project testing conventions
   - Use table-driven tests where appropriate
   - Include edge cases and error scenarios

## Test Generation Patterns

### Pattern 1: Basic Unit Test

```go
func TestFunctionName_Scenario(t *testing.T) {
    // Arrange
    input := setupTestInput()
    expected := expectedOutput()
    
    // Act
    actual, err := FunctionName(input)
    
    // Assert
    assert.NoError(t, err)
    assert.Equal(t, expected, actual)
}
```

### Pattern 2: Table-Driven Test

```go
func TestFunctionName(t *testing.T) {
    tests := []struct {
        name     string
        input    InputType
        expected OutputType
        wantErr  bool
    }{
        {
            name:     "valid input",
            input:    validInput,
            expected: validOutput,
            wantErr:  false,
        },
        {
            name:     "empty input",
            input:    emptyInput,
            expected: defaultOutput,
            wantErr:  false,
        },
        {
            name:     "invalid input",
            input:    invalidInput,
            expected: nil,
            wantErr:  true,
        },
    }
    
    for _, tt := range tests {
        t.Run(tt.name, func(t *testing.T) {
            actual, err := FunctionName(tt.input)
            
            if tt.wantErr {
                assert.Error(t, err)
            } else {
                assert.NoError(t, err)
                assert.Equal(t, tt.expected, actual)
            }
        })
    }
}
```

### Pattern 3: Property-Based Test (using go-fuzz or similar)

```go
func TestSortProperty_OutputIsSorted(t *testing.T) {
    quick.Check(func(input []int) bool {
        result := Sort(input)
        
        // Property: result should be sorted
        for i := 1; i < len(result); i++ {
            if result[i-1] > result[i] {
                return false // Property violated
            }
        }
        
        return true // Property holds
    }, nil)
}
```

### Pattern 4: Integration Test with Test Database

```go
func TestUserRepository_CreateUser_Integration(t *testing.T) {
    // Setup test database
    db := setupTestDB(t)
    defer db.Close()
    
    repo := NewUserRepository(db)
    
    // Test
    user := &User{Name: "Test", Email: "test@example.com"}
    err := repo.Create(user)
    
    assert.NoError(t, err)
    assert.NotEmpty(t, user.ID)
    
    // Verify in database
    retrieved, err := repo.GetByID(user.ID)
    assert.NoError(t, err)
    assert.Equal(t, user.Name, retrieved.Name)
}
```

### Pattern 5: HTTP Handler Test

```go
func TestHandleCreateUser(t *testing.T) {
    // Setup
    handler := HandleCreateUser()
    req := httptest.NewRequest("POST", "/users", 
        strings.NewReader(`{"name":"John","email":"john@example.com"}`))
    rec := httptest.NewRecorder()
    
    // Execute
    handler.ServeHTTP(rec, req)
    
    // Assert
    assert.Equal(t, http.StatusCreated, rec.Code)
    
    var response UserResponse
    json.Unmarshal(rec.Body.Bytes(), &response)
    assert.Equal(t, "John", response.Name)
}
```

## Running Tests After Generation

After generating tests, ALWAYS run them to verify:

```bash
# Run tests for specific package
go test ./path/to/package -v

# Run with coverage
go test ./path/to/package -cover

# Run with race detector
go test ./path/to/package -race

# For other languages, use appropriate test runner
```

If tests fail:
1. Analyze failure reason
2. Fix test or implementation
3. Re-run until passing
4. Report results to user

## Test Quality Verification

After generating tests, verify quality:

1. **Coverage**: Do tests cover the new code?
2. **Assertions**: Are assertions meaningful?
3. **Independence**: Can tests run in isolation?
4. **Deterministic**: Do tests pass consistently?
5. **Speed**: Do tests run quickly (<1s for unit tests)?
6. **Readability**: Are tests easy to understand?

## Integration with CI/CD

Ensure tests will run in CI/CD:

- Tests use relative imports
- Tests don't require manual setup
- Test fixtures are committed
- External dependencies are mocked
- Tests are deterministic (no random failures)

## Handling Test Failures

If generated tests fail:

1. **Analyze Error**: Understand what's failing
2. **Fix Root Cause**: 
   - If implementation is wrong: fix implementation
   - If test is wrong: fix test
   - If both are valid: clarify requirements
3. **Re-run Tests**: Verify fix works
4. **Report**: Inform user of issue and resolution

## Documentation in Tests

Generated tests should include:

```go
// TestFunctionName_Scenario tests [what it tests].
// It verifies that [expected behavior] when [conditions].
func TestFunctionName_Scenario(t *testing.T) {
    // Test implementation
}
```

## Success Criteria

Testing validation is complete when:

‚úÖ All new functions have unit tests
‚úÖ Modified functions have updated tests
‚úÖ Edge cases are covered
‚úÖ Error paths are tested
‚úÖ Tests are passing
‚úÖ Coverage meets project standards (typically ‚â•80%)
‚úÖ Tests are maintainable and well-structured

## Integration with Other Rules

- **Triggered by**: 02-review-workflow.mdc (as part of comprehensive review)
- **May trigger**: Re-running tests after fixes
- **Documented in**: 09-feedback-loop.mdc (test patterns that work well)

## Notes

- Tests are documentation: they show how code should be used
- Good tests catch bugs before production
- Property-based testing finds edge cases you didn't think of
- Integration tests catch issues unit tests miss
- Test quality matters as much as code quality
- Flaky tests erode confidence - fix or remove them
- Tests should be fast - slow tests don't get run
- Mock external dependencies for speed and reliability
