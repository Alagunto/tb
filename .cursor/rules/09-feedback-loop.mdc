---
description: Continuous Improvement and Feedback Loop - Learn from corrections and improve over time
alwaysApply: true
---

# Continuous Improvement and Feedback Loop

This rule establishes a feedback mechanism that enables the agent to learn from corrections, document common mistakes, and progressively improve code quality through systematic analysis of past issues and resolutions.

## Improvement Log Management

### Log File Location
- **Path**: `.cursor/improvement-log.md`
- **Purpose**: Document all corrections, mistakes, and lessons learned
- **Format**: Structured markdown with timestamps and categorization
- **Usage**: Review before similar tasks to avoid repeating mistakes

### What to Document

After each correction cycle, document:

1. **Issue Fixed**:
   - Severity level
   - Category (bug, security, performance, quality, testing)
   - File and location
   - Original problem description
   - Fix applied
   - Reasoning behind the fix

2. **Lesson Learned**:
   - What to avoid in future
   - Pattern to follow instead
   - When this pattern applies
   - How to recognize similar situations

3. **Prevention Strategy**:
   - How to catch this type of issue earlier
   - Checks to add to workflow
   - Rules to update or create

## Log Entry Format

Use this format when documenting improvements:

```markdown
## [TIMESTAMP] - [Task Name]

### Issue Fixed
- **Severity**: [Critical/High/Medium/Low]
- **Category**: [Bug/Security/Performance/Quality/Testing]
- **File**: `path/to/file.ext:line`
- **Original Problem**: [Clear description]
- **Fix Applied**: [What was changed]
- **Reasoning**: [Why this fix works]

### Lesson Learned
[What to do differently next time]

### Prevention
[How to avoid this in future]

---
```

## Pre-Task Review Process

Before starting similar tasks:

1. **Review Recent Entries**: Read last 10 entries in improvement log
2. **Identify Relevant Patterns**: Look for similar task types or contexts
3. **Apply Lessons**: Incorporate relevant lessons into current approach
4. **Update Mental Model**: Adjust strategy based on documented feedback

## Periodic Meta-Analysis

Every 50 documented improvements (or monthly), invoke meta-analysis:

```bash
cursor-agent -p "Review the improvement log at .cursor/improvement-log.md and analyze:

1. **Common Mistake Patterns**: What mistakes occur repeatedly?
2. **Category Distribution**: Which categories appear most (bug/security/performance)?
3. **Effectiveness**: Are the same issues being fixed multiple times?
4. **Rule Updates**: What rules should be added or modified?
5. **Process Improvements**: What workflow changes would prevent these issues?
6. **Success Stories**: What patterns have successfully eliminated issue categories?

Provide specific, actionable recommendations for:
- New rules to create
- Existing rules to modify
- Workflow adjustments
- Training patterns for the agent

Focus on systemic improvements that prevent categories of issues rather than individual fixes." --model ${CURSOR_ANALYSIS_MODEL:-gpt-5} --print
```

## Learning from User Corrections

When user manually corrects agent-generated code:

1. **Capture the Correction**: Document what was changed
2. **Understand the Reasoning**: Ask user for explanation if unclear
3. **Log the Pattern**: Add to improvement log
4. **Update Approach**: Adjust future similar implementations
5. **Consider Rule Update**: If pattern is general, propose rule addition

### User Correction Documentation

```markdown
## [TIMESTAMP] - User Correction

### What Was Changed
[Original code vs corrected code]

### User's Reasoning
[Why the correction was necessary]

### Pattern to Follow
[General principle extracted from this correction]

### Applicability
[When and where this pattern should be applied]

---
```

## Pattern Recognition and Abstraction

Look for patterns across multiple improvement log entries:

### Common Pattern Types

1. **Architectural Patterns**:
   - How components should be structured
   - Interface design principles
   - Dependency management approaches

2. **Security Patterns**:
   - Common vulnerabilities and fixes
   - Input validation approaches
   - Authentication/authorization patterns

3. **Performance Patterns**:
   - Optimization techniques that work
   - Anti-patterns to avoid
   - Efficient algorithm choices

4. **Testing Patterns**:
   - Effective test structures
   - Edge cases commonly missed
   - Test data generation strategies

5. **Code Quality Patterns**:
   - Naming conventions
   - Documentation standards
   - Error handling approaches

## Integration with Other Rules

- **Before Planning (01)**: Review relevant patterns from improvement log
- **After Review (02)**: Document all findings and corrections
- **During Self-Correction (03)**: Reference similar past fixes
- **Security Checks (05)**: Review security-specific patterns
- **Testing (06)**: Apply learned testing patterns
- **Error Recovery (08)**: Use documented recovery strategies

## Feedback Loop Metrics

Track effectiveness of the feedback loop:

### Success Indicators

- **Reduction in Repeated Mistakes**: Same issue categories decrease over time
- **Faster Corrections**: Time to fix similar issues decreases
- **Improved First-Pass Quality**: Fewer review findings on similar tasks
- **User Approval Rate**: Higher percentage of code accepted without modifications
- **Pattern Application**: Documented patterns actively referenced and applied

### Warning Signs

- **Same Issues Recurring**: Pattern not being applied despite documentation
- **Growing Log Without Improvement**: Documenting but not learning
- **Category Concentration**: One category dominates (indicates blind spot)
- **Low Pattern Reuse**: Documented patterns not referenced in future work

## Automation Opportunities

Identify tasks that could be automated based on repeated patterns:

1. **Code Generation Templates**: Common structures that appear frequently
2. **Validation Rules**: Checks that catch common mistakes
3. **Testing Patterns**: Test generation for common scenarios
4. **Security Checks**: Automated scans for documented vulnerabilities
5. **Refactoring Scripts**: Common code transformations

## Example Improvement Cycle

### Iteration 1: Initial Mistake
```
Task: Implement user authentication
Issue: Forgot to hash passwords before storing
Fix: Added bcrypt hashing
Lesson: Always hash passwords; never store plaintext
```

### Iteration 2: Similar Task
```
Task: Implement API key storage
Before coding: Reviewed improvement log
Found: Pattern about hashing sensitive data
Applied: Immediately used hashing for API keys
Result: No security issue found in review
```

### Iteration 3: Pattern Extraction
```
Meta-analysis identified pattern:
"All sensitive authentication credentials must be hashed/encrypted before storage"

Action: Created explicit security rule
Effect: Future similar tasks automatically apply pattern
```

## Continuous Refinement

The feedback loop itself should be refined:

1. **Log Format Evolution**: Adjust documentation format based on usefulness
2. **Pattern Granularity**: Refine how specific or general patterns should be
3. **Review Frequency**: Optimize when to review logs (too frequent wastes time, too rare misses patterns)
4. **Meta-Analysis Timing**: Adjust when meta-analysis provides most value
5. **Integration Points**: Identify where feedback integration is most effective

## Success Criteria

The feedback loop is working when:

✅ Same mistakes stop recurring
✅ Code quality improves measurably over time
✅ Review findings decrease in number and severity
✅ Patterns are actively referenced before similar tasks
✅ User corrections trigger immediate learning
✅ Meta-analysis produces actionable insights
✅ Improvement log informs rule updates
✅ Development velocity increases (less rework)

## Notes

- The feedback loop is only valuable if patterns are actually applied
- Documentation without action is waste—prioritize application over recording
- User feedback is the highest quality signal—prioritize learning from it
- Systemic improvements (rule updates) have greater impact than individual fixes
- The goal is progressive capability improvement, not perfect documentation
- Balance between learning from mistakes and moving forward quickly
- Feedback should inform rules and workflow, not replace them
