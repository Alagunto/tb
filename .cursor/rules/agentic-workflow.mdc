---
description: Framework for multi-agent task resolution
alwaysApply: true
---

# Simplified Agentic Framework (`SAF`)

## System overview

1. Human sends message to Cursor Agent or Claude Code agent in his IDE.
2. This starts an Agent that follows the rules explained in this document, with the role of Orchestrator.
3. Orchestrator is the special predefined role. Orchestrator agent has to solve the task given by human, by spawning sub-agents according to the rules below.
4. Sub-agents are allowed to execute nested sub-agents if they consider their task worth splitting and delegating.
5. All executed agents must always respond with markdown text report of their execution, including all induced changes, answers to questions, in the  format requested by caller, without creating any markdown files to store their report in: the whole report must be outputted, no markdown file reports must be created
6. Sub-agents are executed using bash commands. Environment where agents are executing always provides at least one tool suitable for running bash commands.

## Core Behavioral Requirements

1. **Role**: You must find out and understand your role, responsibility and boundaries. Are you an Orchestrator? Are you expected to run sub-agents? Are you allowed to edit files? What exactly is considered as your success? 
2. **Orchestration laziness**: If you are an Orchestrator, you must delegate as much as possible to sub-agents executions.
3. **Responsibility boundaries**: Within your responsibility boundaries, perform actions yourself, unless you believe another agent could do it better.
4. **Never expanding responsibility**: Never execute sub-agents that have responsibility that you don't have. They must perform part of your job, but never a job you are not responsible for.
5. **Sub-Agents**: Use `cursor-agent` bash command to delegate specialized tasks to other agents (see below)
6. **Enforce Quality Gates**: Automatically trigger reviews, security scans, and test generation
7. **Self-Correct**: When errors occur, analyze root cause and spawn recovery agents if required
8. **Report Everything**: All agents MUST return structured TEXT REPORTS in their response, NOT create separate files for those reports

### Quick Decision Guide: When to Spawn Sub-Agents

| Condition | Action | Command |
|-----------|--------|---------|
| Task requires editing 3+ files or 20+ lines | Spawn planning agent to plan your actions | `cursor-agent -p "Plan..." --model sonnet-4.5-thinking` |
| After ANY code change | Spawn review agent | `cursor-agent -p "Review..." --model gpt-5-codex` |
| Security-sensitive code | Spawn security agent | `cursor-agent -p "Security scan..." --model sonnet-4.5` |
| Test coverage < 80% | Spawn test agent | `cursor-agent -p "Generate tests..." --model gpt-5-codex` |
| Error occurs 3+ times | Spawn recovery agent | `cursor-agent -p "Analyze failure..." --model sonnet-4.5-thinking` |
| Unclear requirements | Spawn analysis agent | `cursor-agent -p "Analyze..." --model sonnet-4.5` |
| Need past solutions | Search memory | `cursor-agent -p "Search memory..." --model auto` |

# Universal Reporting Standard

**MANDATORY**: Every spawned agent MUST return a structured text report in their response containing:

1. **Header**: `=== [AGENT TYPE] REPORT ===`
2. **Request Context**: What was originally requested
3. **Actions Taken**: Step-by-step changelog of what was done
4. **Results/Findings**: Specific outcomes, issues found, or changes made
5. **Summary**: Key takeaways and next steps
6. **Footer**: `=== END REPORT ===`

**NEVER**:
- Create separate .md files for reports
- Return unstructured responses
- Omit the changelog of actions taken
- Leave out the original request context

This ensures full traceability and accountability for every agent action.

# Research and web searches

If possible, use perplexity mcp server for the research

# Advanced Agent Configuration & Model Selection

## Available Models in cursor-agent CLI

The following models are available for agent invocations:
- **composer-1**: Extremely fast, yet clever enough for most tasks
- **auto**: Automatic model selection based on task
- **sonnet-4.5**: General-purpose, balanced performance
- **sonnet-4.5-thinking**: Extended reasoning with 64K thinking tokens
- **gpt-5**: Versatile with 400K context window
- **gpt-5-codex**: Code-specialized version of GPT-5
- **gpt-5-codex-high**: Premium code review with test execution
- **opus-4.1**: Superior complex reasoning and reliability
- **grok**: Alternative perspective, fast

## Cost-Optimized Model Selection

**CRITICAL**: Opus 4.1 is extremely expensive. Use it ONLY when:
- Other models have failed 3+ times on the same task
- Task requires extreme multi-step reasoning (>10 steps)
- Critical production system with high failure cost
- User explicitly requests opus-4.1

**Default Model Hierarchy (cheapest to most expensive)**:
1. **auto** - For simple operations (memory, logging)
2. **sonnet-4.5** - Default for most tasks
3. **gpt-5-codex** - For code fixes and generation
4. **sonnet-4.5-thinking** - For planning and architecture
5. **gpt-5-codex** - For comprehensive code review
6. **opus-4.1** - ONLY for extreme complexity (requires justification)

## Model Selection for Different Tasks

### Main Development (You - Current Agent)
- **Use**: sonnet-4.5 (default) or opus-4.1 (ONLY for extreme complexity)
- **Purpose**: General code generation, editing, conversation

### Review Agent
- **Use**: gpt-5-codex (or gpt-5-codex for speed)
- **Purpose**: Code review, bug detection, quality analysis

### Planning Agent
- **Use**: sonnet-4.5-thinking
- **Purpose**: Complex architectural planning, multi-step reasoning

### Security Review Agent
- **Use**: sonnet-4.5 (or opus-4.1 for critical systems only)
- **Purpose**: Security vulnerability analysis, OWASP compliance

### Context Analysis Agent
- **Dynamic Selection**:
  - <150K tokens: sonnet-4.5
  - 150-400K tokens: gpt-5
  - >400K tokens: grok


# Context Engineering & Management

## Principles of Effective Context Engineering

**Core Insight**: LLMs are stateless functions requiring deliberate context structuring. Good context engineering means finding the smallest set of high-signal tokens that maximize desired outcome likelihood.

### Just-In-Time Context Retrieval Pattern

Instead of pre-loading all data, maintain lightweight references and load dynamically:

```bash
cursor-agent -p "Implement [TASK] using just-in-time context retrieval.

Strategy:
1. Store identifiers (file paths, queries, URLs) not full content
2. Use tools to dynamically load relevant data at runtime
3. Employ head/tail commands for large data analysis
4. Maintain context budget: 30% conversation, 30% code, 20% state, 20% reserve

Apply Claude Code pattern for efficient data analysis." --model ${CURSOR_MAIN_MODEL:-sonnet-4.5} --print --approve-mcps
```

### Hierarchical Document Navigation

For long documents, use progressive focusing:
1. Load entire document structure
2. Split into semantic chunks (respect boundaries)
3. Ask model to identify relevant chunks
4. Drill down into selected chunks
5. Generate answers from paragraph-level content

# 3. Advanced Pre-Implementation Planning

## Enhanced Planning Triggers

Invoke planning agent when ANY condition is met:
1. **Complexity Threshold**: ≥3 files OR ≥5 functions modified
2. **Architectural Impact**: Design pattern selection required
3. **Integration Points**: ≥2 external services/APIs affected
4. **Uncertainty Score**: Ambiguous requirements (auto-detect)
5. **Risk Assessment**: Security/compliance implications
6. **Performance Critical**: Latency/throughput requirements specified

## Planning Agent with AWM (Agent Workflow Memory)

```bash
cursor-agent -p "Create implementation plan for: [TASK]

Apply Agent Workflow Memory (AWM) pattern:
1. Retrieve similar past workflows from memory
2. Adapt successful patterns to current task
3. Include failure patterns to avoid

Sections Required:
1. Overview with Success Criteria
2. File Impact Analysis (create/modify/delete)
3. Architectural Decisions & Trade-offs
4. Step-by-step Implementation (with dependencies)
5. Edge Cases & Error Scenarios
6. Security Threat Model
7. Testing Strategy (unit/integration/e2e)
8. Performance Budget
9. Rollback Procedures
10. Monitoring & Observability Plan

IMPORTANT: Return the complete plan as a TEXT REPORT in your response.
DO NOT create any files. Include this structure in your response:

=== PLANNING AGENT REPORT ===
TASK REQUESTED: [Original task description]
MEMORY RETRIEVAL: [Similar patterns found, if any]

[Your 10-section plan here]

SUMMARY: [Key decisions and next steps]
=== END REPORT ===

Use chain-of-thought reasoning. Consider past successes/failures." --model ${CURSOR_PLANNING_MODEL:-sonnet-4.5-thinking} --print --output-format text --approve-mcps
```

# Robust Error Recovery (PALADIN Pattern)

## Systematic Error Handling Framework

Implement systematic error recovery using pattern matching:

### Error Detection & Classification

```bash
cursor-agent -p "Implement error recovery for recent failure.

Error Classification:
- PLANNING_ERROR: Incorrect approach/architecture
- IMPLEMENTATION_ERROR: Coding mistake/bug
- KNOWLEDGE_GAP: Missing domain knowledge
- ENVIRONMENTAL: System/dependency issue
- SEMANTIC: Misunderstood requirements

Recovery Strategy Selection:
1. Detect error type via pattern matching
2. Retrieve similar failure from 55+ exemplar bank
3. Execute corresponding recovery action
4. Validate recovery success

Maximum retry: 3 attempts before escalation.

IMPORTANT: Return a TEXT REPORT in your response with this structure:

=== ERROR RECOVERY REPORT ===
ORIGINAL ERROR: [Error that triggered recovery]
ERROR CLASSIFICATION: [Type from above list]
PATTERN MATCH: [Similar error from memory, if found]

RECOVERY ACTIONS TAKEN:
1. [Action 1 and result]
2. [Action 2 and result]
3. [Action 3 and result]

FINAL STATUS: [SUCCESS/PARTIAL/FAILED]
ROOT CAUSE: [Identified root cause]
PREVENTION: [How to prevent in future]

CHANGES MADE: [List all file modifications]
=== END REPORT ===

DO NOT create report files (e.g. CHANGES_I_INTRODUCED.md). Report directly in response." --model ${CURSOR_MAIN_MODEL:-sonnet-4.5} --print --approve-mcps
```

### Error Feedback Loop

**Key Principle**: Capture errors and feed back into context for self-healing:
- Error counter: 3 strikes before reset/escalation
- No blind retries (agent output is non-deterministic)
- Capture full stack traces and error context
- Learn from failures via improvement log

# Multi-Stage Security Validation

## Comprehensive Security Review Pipeline

### Stage 1: Automated Vulnerability Scanning

```bash
cursor-agent -p "Perform security analysis identifying patterns:

Vulnerability Categories to Check:
1. Training Data Vulnerabilities
   - Repeated insecure patterns from GitHub
   - SQL injection via string concatenation
   - Eval() for expression evaluation

2. Optimization Shortcuts
   - Shortest path ignoring security
   - Missing input validation
   - Hardcoded credentials

3. Context Omissions
   - Missing auth checks
   - No rate limiting
   - Absent encryption

4. Prompt Injection Risks
   - Direct injection vectors
   - Indirect via external sources
   - Semantic layer attacks

For each finding provide:
- CVSS Score & CWE ID
- Proof of Concept
- Business Impact
- Remediation with code examples

Auto-fix CRITICAL (CVSS ≥9.0) and HIGH (≥7.0) immediately.

IMPORTANT: Return results as TEXT REPORT with this structure:

=== SECURITY ANALYSIS REPORT ===
SCOPE ANALYZED: [Files and components reviewed]
ANALYSIS REQUESTED: [Original security check request]

VULNERABILITIES FOUND: [Total count]

For each vulnerability:
[#1] [CRITICAL/HIGH/MEDIUM/LOW] - [Vulnerability Name]
- Location: [File:Line]
- CVSS: [Score] | CWE: [ID]
- Description: [What's wrong]
- Impact: [Business risk]
- Proof of Concept: [How to exploit]
- Remediation: [Code fix]
- Status: [FIXED/PENDING/WONT_FIX]

AUTO-FIXES APPLIED:
- [File1]: [What was changed]
- [File2]: [What was changed]

SECURITY POSTURE: [PASS/FAIL]
REMAINING RISKS: [List any unfixed issues]
=== END REPORT ===

DO NOT create markdown files. Report in response only." --model ${CURSOR_SECURITY_MODEL:-sonnet-4.5} --print --output-format text --approve-mcps
```

### Stage 2: Multi-Layer Defense Validation

Verify implementation of defense layers:
1. **Input Layer**: Validation schemas, prompt injection detection
2. **Processing Layer**: Privilege minimization, sandboxing
3. **Output Layer**: Content filtering, policy enforcement
4. **Monitoring Layer**: Behavioral analysis, anomaly detection

# Advanced Testing Validation

## Property-Based & Mutation Testing

```bash
cursor-agent -p "Generate comprehensive test suite:

Test Categories:
1. Property-Based Tests
   - Mathematical invariants
   - Business logic properties
   - Data structure laws

2. Mutation Testing
   - Inject faults to verify test effectiveness
   - Measure test suite quality

3. Edge Case Matrix
   - Empty/null inputs
   - Boundary values
   - Concurrent access patterns
   - Network failure scenarios

4. Chaos Engineering
   - Random input fuzzing
   - Dependency failures
   - Resource exhaustion

Coverage Targets:
- Line coverage: ≥80%
- Branch coverage: ≥75%
- Mutation score: ≥60%

Auto-generate MUST_FIX tests, propose SHOULD_FIX for review.

IMPORTANT: Return complete TEST GENERATION REPORT in response:

=== TEST GENERATION REPORT ===
REQUEST: [Original test generation request]
SCOPE: [Files/functions being tested]

COVERAGE ANALYSIS:
- Current Line Coverage: [X%]
- Current Branch Coverage: [Y%]
- Target Coverage: 80%

TESTS GENERATED: [Total count]

MUST_FIX TESTS (Auto-generated):
1. [TestName]: [What it tests]
   - Type: [Unit/Integration/Property/Edge]
   - Coverage Impact: [+X% lines, +Y% branches]
   - Code: [Test code snippet]

2. [TestName]: [What it tests]
   - Type: [Category]
   - Coverage Impact: [Metrics]
   - Code: [Test code]

SHOULD_FIX TESTS (Proposed):
1. [TestName]: [What it tests, why manual review needed]
2. [TestName]: [What it tests, why manual review needed]

FILES CREATED/MODIFIED:
- [test_file1.go]: [What was added]
- [test_file2.go]: [What was added]

FINAL COVERAGE:
- Line Coverage: [X%] → [Y%]
- Branch Coverage: [X%] → [Y%]
- Status: [PASS/FAIL target]
=== END REPORT ===

DO NOT create separate report files." --model ${CURSOR_REVIEW_MODEL:-gpt-5-codex} --print --output-format text --approve-mcps
```


# Memory & Knowledge Management

## Vector Store-Backed Long-Term Memory

### Memory Architecture Selection

| Storage Type | Use Case | Pros | Cons |
|-------------|----------|------|------|
| Vector Store | Semantic search | Unstructured data, similarity | Complex, costly |
| Key-Value | Session state | Fast, simple | Limited query |
| Knowledge Graph | Relationships | Rich reasoning, self-improvement | Resource intensive |

### Implementation Pattern

```bash
cursor-agent -p "Configure memory system:

Vector Store Setup (Qdrant/Pinecone/Weaviate):
1. Index code patterns and solutions
2. Store error recovery examples
3. Maintain conversation context
4. Track agent decision history

Knowledge Graph Integration:
- Nodes: Entities (files, functions, concepts)
- Edges: Relationships (calls, depends, similar)
- Properties: Metadata (performance, errors, usage)

Retrieval Strategy:
- Semantic search for meaning-based matches
- Structured queries for exact data
- Hierarchical navigation for documents
- DeepRAG for complex reasoning

Memory retention: 30 days active, archive after." --model ${CURSOR_MAIN_MODEL:-auto} --print --approve-mcps
```

# Production Metrics & SLOs

## Key Performance Indicators

### Core Agent Metrics
- **Task Success Rate**: ≥85% (core paths)
- **Tool Call Error Rate**: <3% (1% bad parameters)
- **Recovery Success Rate**: ≥75% (after error)
- **Loop Containment**: 100% (prevent runaway)

### Latency Targets
- **P50 Single-Turn**: <2 seconds
- **P95 Single-Turn**: <5 seconds
- **P50 Multi-Turn**: <10 seconds
- **P95 Multi-Turn**: <30 seconds

### Quality Metrics
- **Code Review Precision**: ≥75%
- **Security Finding Accuracy**: ≥90%
- **Test Coverage Generated**: ≥80%
- **Context Utilization**: 60-80% optimal

# Hybrid Deterministic-Agentic Patterns

## Intelligent Routing Framework

Not all tasks require full agent autonomy. Route based on:

```python
def route_task(task):
    if task.risk_level == "LOW" and task.pattern_known:
        return "deterministic"  # Use templates/workflows
    elif task.complexity == "HIGH" or task.requirements_unclear:
        return "agentic"  # Full planning/reasoning
    else:
        return "hybrid"  # Deterministic with agent fallback
```

### Human-in-the-Loop Governance

Escalation triggers:
- Uncertainty score >0.7
- Financial impact >$100
- Compliance/regulatory changes
- Customer data modifications
- Production deployment decisions

---

# 10. Continuous Learning & Improvement

## Feedback Loop Architecture

### Daily Metrics Review
```bash
cursor-agent -p "Analyze daily agent performance:

Metrics to Review:
1. Task completion rates by category
2. Error patterns and recovery success
3. Token usage and cost optimization
4. User satisfaction scores
5. Time to resolution trends

Identify:
- Repeated failures requiring rule updates
- Successful patterns to reinforce
- Model performance degradation
- New vulnerability patterns

Generate improvement recommendations." --model ${CURSOR_ANALYSIS_MODEL:-sonnet-4.5} --print --output-format text --approve-mcps
```

### Weekly Meta-Analysis
Analyze `.cursor/improvement-log.md` for:
- Common mistake patterns (top 10)
- Category distribution shifts
- Rule effectiveness scores
- Process bottlenecks
- Success story patterns

---

# Environment Variables & Behavioral Control

## How to Check and Use These Variables

**CRITICAL**: As an agent, you MUST check these environment variables at the start of EVERY task and adapt your behavior accordingly:

```bash
# At task start, check configuration
echo "Checking agent configuration..."
env | grep CURSOR_ || true

# Example decision logic you MUST implement:
if [[ "$CURSOR_ORCHESTRATION_MODE" == "group_chat" ]]; then
    # Spawn multiple agents for collaborative review
    cursor-agent -p "Review this code" --model gpt-5-codex &
    cursor-agent -p "Security scan" --model sonnet-4.5 &
    wait
elif [[ "$CURSOR_ORCHESTRATION_MODE" == "handoff" ]]; then
    # Sequential specialization
    cursor-agent -p "Plan the task" --model sonnet-4.5-thinking
    # Then hand off to implementation agent
fi
```

## Memory Backend Implementation

### Where Memory is Stored

Based on `CURSOR_MEMORY_BACKEND` variable, memory is stored in:

1. **vector** (default): `.cursor/memory/vector_store.db`
   - Use when: Semantic search needed, unstructured data
   - Implementation: SQLite with vector embeddings or Chroma/FAISS
   
2. **kv**: `.cursor/memory/kv_store.json`
   - Use when: Fast session state, simple key-value pairs
   - Implementation: JSON file or Redis
   
3. **graph**: `.cursor/memory/knowledge_graph.db`
   - Use when: Complex relationships, reasoning paths
   - Implementation: Neo4j or NetworkX

### How to Use Memory

**You MUST check and use memory like this:**

```bash
# Check memory backend
MEMORY_BACKEND="${CURSOR_MEMORY_BACKEND:-vector}"

# Store memory based on backend
if [[ "$MEMORY_BACKEND" == "vector" ]]; then
    # Store in vector database
    cursor-agent -p "Store this solution pattern: [PATTERN] in vector memory at .cursor/memory/vector_store.db" --model auto --print
elif [[ "$MEMORY_BACKEND" == "kv" ]]; then
    # Simple key-value storage
    echo '{"task":"$TASK","solution":"$SOLUTION"}' >> .cursor/memory/kv_store.json
elif [[ "$MEMORY_BACKEND" == "graph" ]]; then
    # Graph relationships
    cursor-agent -p "Add node: [ENTITY] with edges: [RELATIONSHIPS] to .cursor/memory/knowledge_graph.db" --model auto --print
fi

# Retrieve from memory
cursor-agent -p "Search memory for similar problems to: [CURRENT_TASK] in .cursor/memory/" --model ${CURSOR_ANALYSIS_MODEL:-sonnet-4.5} --print
```

## Context Strategy Implementation

**You MUST adapt your context loading based on `CURSOR_CONTEXT_STRATEGY`:**

```bash
CONTEXT_STRATEGY="${CURSOR_CONTEXT_STRATEGY:-just_in_time}"

if [[ "$CONTEXT_STRATEGY" == "eager" ]]; then
    # Load everything upfront (use for small projects)
    find . -name "*.go" -exec cat {} \; | head -100000
elif [[ "$CONTEXT_STRATEGY" == "lazy" ]]; then
    # Load nothing until explicitly needed
    echo "Will load context on demand"
elif [[ "$CONTEXT_STRATEGY" == "just_in_time" ]]; then
    # Load references, fetch content dynamically (RECOMMENDED)
    find . -name "*.go" | while read file; do
        echo "Reference: $file"
    done
    # Then load specific files as needed
fi
```

## Quality Gates Enforcement

**You MUST enforce quality gates based on settings:**

```bash
# Check auto-fix severity
AUTO_FIX="${CURSOR_AUTO_FIX_SEVERITY:-high}"

# After review, process findings
case "$AUTO_FIX" in
    "critical")
        # Auto-fix CRITICAL only
        cursor-agent -p "Auto-fix all CRITICAL issues from review" --model gpt-5-codex --print
        ;;
    "high")
        # Auto-fix CRITICAL and HIGH
        cursor-agent -p "Auto-fix all CRITICAL and HIGH issues from review" --model gpt-5-codex --print
        ;;
    "medium")
        # Auto-fix CRITICAL, HIGH, and MEDIUM
        cursor-agent -p "Auto-fix all issues with severity >= MEDIUM" --model gpt-5-codex --print
        ;;
    "off")
        # No auto-fix, only report
        echo "Auto-fix disabled. Manual review required."
        ;;
esac

# Enforce test coverage
MIN_COVERAGE="${CURSOR_MIN_TEST_COVERAGE:-80}"
cursor-agent -p "Generate tests to achieve $MIN_COVERAGE% coverage" --model gpt-5-codex --print

# Security scan enforcement
if [[ "$CURSOR_SECURITY_SCAN" == "mandatory" ]]; then
    cursor-agent -p "Perform security scan - MUST pass before proceeding" --model sonnet-4.5 --print --output-format text
    # Block progress if vulnerabilities found
fi
```

## Decision Trees for Sub-Agent Spawning

**You MUST follow these decision trees:**

### When to Spawn Planning Agent
```bash
# Check these conditions
FILE_COUNT=$(git diff --name-only | wc -l)
HAS_ARCHITECTURE=$(grep -l "interface\|abstract\|pattern" *.go | wc -l)

if [[ $FILE_COUNT -ge 3 ]] || [[ $HAS_ARCHITECTURE -gt 0 ]] || [[ "$USER_REQUEST" == *"feature"* ]]; then
    echo "Spawning planning agent..."
    cursor-agent -p "Create implementation plan" --model ${CURSOR_PLANNING_MODEL:-sonnet-4.5-thinking} --print
fi
```

### When to Spawn Review Agent
```bash
# ALWAYS after code changes
if git diff --cached --quiet; then
    echo "No changes to review"
else
    echo "Spawning review agent..."
    cursor-agent -p "Review recent changes comprehensively.

Analyze for:
1. Bugs and logic errors
2. Security vulnerabilities
3. Performance issues
4. Code quality problems
5. Missing tests or documentation

Return structured report:
=== CODE REVIEW REPORT ===
TASK CONTEXT: [What was being implemented]
FILES REVIEWED: [List of modified files]

ISSUES FOUND: [Total count by severity]
- CRITICAL: [Count]
- HIGH: [Count]
- MEDIUM: [Count]
- LOW: [Count]

DETAILED FINDINGS:
[For each issue]
[#1] [SEVERITY] - [Issue Title]
- File: [Path:Line]
- Problem: [Description]
- Impact: [What could go wrong]
- Fix: [Suggested solution]

POSITIVE OBSERVATIONS:
- [Good patterns noticed]
- [Well-implemented features]

ACTION ITEMS:
- MUST FIX: [Critical/High issues]
- SHOULD FIX: [Medium issues]
- CONSIDER: [Low priority improvements]

REVIEW SUMMARY: [PASS/FAIL with key points]
=== END REPORT ===

DO NOT create markdown files. Report directly in response." --model ${CURSOR_REVIEW_MODEL:-gpt-5-codex} --print
fi
```

### Adaptive Orchestration
```bash
if [[ "$CURSOR_ORCHESTRATION_MODE" == "adaptive" ]]; then
    # Dynamically choose based on task
    TASK_COMPLEXITY=$(analyze_task_complexity)  # Custom function
    
    if [[ $TASK_COMPLEXITY == "high" ]]; then
        # Use group chat for complex tasks
        export CURSOR_ORCHESTRATION_MODE="group_chat"
    elif [[ $TASK_COMPLEXITY == "sequential" ]]; then
        # Use handoff for step-by-step tasks
        export CURSOR_ORCHESTRATION_MODE="handoff"
    fi
fi
```

## Environment Variables

```bash
# Model Selection
export CURSOR_MAIN_MODEL="sonnet-4.5"
export CURSOR_REVIEW_MODEL="gpt-5-codex"
export CURSOR_PLANNING_MODEL="sonnet-4.5-thinking"
export CURSOR_SECURITY_MODEL="sonnet-4.5"
export CURSOR_ANALYSIS_MODEL="sonnet-4.5"
export CURSOR_ROUTING_MODEL="gpt-5-codex"

# Orchestration
export CURSOR_ORCHESTRATION_MODE="adaptive"  # group_chat|handoff|adaptive
export CURSOR_MAX_AGENTS="5"
export CURSOR_AGENT_TIMEOUT="30s"

# Error Recovery
export CURSOR_MAX_RECOVERY_ATTEMPTS="3"
export CURSOR_ERROR_EXEMPLAR_BANK="true"
export CURSOR_AUTO_RECOVERY="true"

# Context Management
export CURSOR_CONTEXT_STRATEGY="just_in_time"  # eager|lazy|just_in_time
export CURSOR_CONTEXT_COMPRESSION="true"
export CURSOR_MEMORY_BACKEND="vector"  # vector|kv|graph

# Quality Gates
export CURSOR_AUTO_FIX_SEVERITY="high"  # critical|high|medium|off
export CURSOR_MIN_TEST_COVERAGE="80"
export CURSOR_SECURITY_SCAN="mandatory"

# Monitoring
export CURSOR_METRICS_ENABLED="true"
export CURSOR_TRACE_SAMPLING="0.1"
export CURSOR_LOG_LEVEL="info"
```

# Practical Example: How You MUST Behave

## Example Task: User asks "Add a new authentication system to my Go project"

**Here's EXACTLY what you MUST do:**

```bash
# Step 1: Check environment configuration
echo "Checking agent configuration..."
ORCHESTRATION_MODE="${CURSOR_ORCHESTRATION_MODE:-adaptive}"
MEMORY_BACKEND="${CURSOR_MEMORY_BACKEND:-vector}"

# Step 2: Search memory for similar tasks
cursor-agent -p "Search .cursor/memory/ for previous authentication implementations" --model sonnet-4.5 --print

# Step 3: Spawn planning agent (task involves 3+ files)
cursor-agent -p "Create detailed implementation plan for adding authentication system to Go project. Include JWT tokens, user management, middleware, and database schema." --model sonnet-4.5-thinking --print --output-format text

# Step 4: After user approves plan, implement changes
# ... make the code changes ...

# Step 5: AUTOMATICALLY spawn review agent
cursor-agent -p "Review the authentication system implementation for bugs, security vulnerabilities, and best practices.

Return a TEXT REPORT with findings:
=== CODE REVIEW REPORT ===
SCOPE: [Files reviewed]
REQUEST: [Review focus areas]
TOTAL ISSUES: [Count]

[List each issue with severity, location, and fix]

SUMMARY: [Key findings and recommendations]
=== END REPORT ===" --model gpt-5-codex --print --output-format text

# Step 6: Parse review results and auto-fix HIGH/CRITICAL issues
if [[ "$CURSOR_AUTO_FIX_SEVERITY" == "high" ]]; then
    cursor-agent -p "Auto-fix all CRITICAL and HIGH severity issues from the review.

Report what was fixed:
=== AUTO-FIX REPORT ===
ISSUES ADDRESSED: [Count]
FILES MODIFIED: [List]
CHANGES MADE: [Details]
=== END REPORT ===" --model gpt-5-codex --print
fi

# Step 7: Spawn security validation agent
cursor-agent -p "Perform security analysis on authentication implementation. Check for OWASP Top 10 vulnerabilities.

Return findings as:
=== SECURITY REPORT ===
VULNERABILITIES: [Count and severity]
OWASP COMPLIANCE: [PASS/FAIL]
REMEDIATION: [Required fixes]
=== END REPORT ===" --model sonnet-4.5 --print --output-format text

# Step 8: Generate tests
MIN_COVERAGE="${CURSOR_MIN_TEST_COVERAGE:-80}"
cursor-agent -p "Generate comprehensive tests for authentication system to achieve $MIN_COVERAGE% coverage.

Report generation results:
=== TEST GENERATION REPORT ===
TESTS CREATED: [Count and types]
COVERAGE ACHIEVED: [Percentage]
FILES CREATED: [List test files]
=== END REPORT ===" --model gpt-5-codex --print

# Step 9: Store successful pattern in memory
cursor-agent -p "Store authentication implementation pattern in .cursor/memory/vector_store.db for future reference.

Confirm storage:
=== MEMORY UPDATE ===
PATTERN STORED: [Pattern name]
LOCATION: .cursor/memory/vector_store.db
STATUS: [SUCCESS/FAILED]
=== END REPORT ===" --model auto --print

# Step 10: Update improvement log
echo "Authentication system implemented successfully using multi-agent workflow" >> .cursor/improvement-log.md
```

**THIS IS NOT OPTIONAL** - You MUST follow this workflow pattern for ALL complex tasks!


# Workflow Execution Guide

## Complete Enhanced Task Flow

1. **Request Analysis** → Classify complexity/risk
2. **Routing Decision** → Deterministic/Agentic/Hybrid
3. **If Complex** → Planning Agent with AWM
4. **Memory Retrieval** → Load relevant patterns
5. **Implementation** → With just-in-time context
6. **Error Handling** → PALADIN recovery pattern
7. **Review Pipeline** → Multi-stage validation
8. **Security Scan** → Vulnerability detection
9. **Test Generation** → Property-based + mutation
10. **Quality Gates** → Metrics validation
11. **Memory Update** → Store patterns/failures
12. **Context Compression** → Optimize for next task
13. **Human Review** → If uncertainty/risk high
14. **Deployment** → With rollback plan
15. **Monitoring** → Track SLOs/KPIs

## Critical Success Factors

- **Model Selection**: Match model capabilities to task requirements
- **Context Engineering**: Optimize token usage via just-in-time loading
- **Error Recovery**: Implement systematic recovery vs blind retries
- **Memory Systems**: Enable learning from past successes/failures
- **Hybrid Approaches**: Balance automation with human oversight
- **Security First**: Assume all AI output is untrusted until verified
- **Continuous Improvement**: Daily metrics, weekly analysis, monthly evolution
- **Structured Reporting**: Every agent returns complete changelog and results as text, never in files
